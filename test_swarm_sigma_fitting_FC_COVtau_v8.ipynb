{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b14d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import expm, solve_sylvester\n",
    "from scipy.signal import correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe50c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "### USEFUL FUNCTIONS ###\n",
    "\n",
    "def hopf_int(gC, f_diff, sigma):\n",
    "    \"\"\"\n",
    "    Computes the linearized Hopf model.\n",
    "    \n",
    "    Parameters:\n",
    "        gC: Connectivity matrix\n",
    "        f_diff: Frequency differences\n",
    "        sigma: Noise variance\n",
    "    Returns:\n",
    "        FC: Functional connectivity matrix\n",
    "        CV: Covariance matrix\n",
    "        Cvth: Full covariance matrix\n",
    "        A: Jacobian matrix\n",
    "    \"\"\"\n",
    "    a = -0.02\n",
    "    N = gC.shape[0]\n",
    "    wo = f_diff * (2 * np.pi)\n",
    "\n",
    "    Cvth = np.zeros((2 * N, 2 * N))\n",
    "\n",
    "    # Jacobian\n",
    "    s = np.sum(gC, axis=1)\n",
    "    B = np.diag(s)\n",
    "\n",
    "    Axx = a * np.eye(N) - B + gC\n",
    "    Ayy = Axx.copy()\n",
    "    Axy = -np.diag(wo)\n",
    "    Ayx = np.diag(wo)\n",
    "\n",
    "    A = np.block([[Axx, Axy], [Ayx, Ayy]])\n",
    "\n",
    "    # Noise covariance matrix\n",
    "    if np.isscalar(sigma):  # Homogeneous noise\n",
    "        Qn = np.diag([sigma**2] * (2 * N))\n",
    "    else:  # Heterogeneous noise\n",
    "        Qn = np.diag(np.concatenate([sigma**2, sigma**2]))\n",
    "\n",
    "    # Solve Sylvester equation\n",
    "    Cvth = solve_sylvester(A, A.T, -Qn)\n",
    "\n",
    "    # Correlation from covariance\n",
    "    FCth = np.corrcoef(Cvth)\n",
    "    FC = FCth[:N, :N].copy()\n",
    "    CV = Cvth[:N, :N].copy()\n",
    "\n",
    "    return FC, CV, Cvth, A\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def plot_FC_matrices(FC1, FC2, title1=\"FC Matrix 1\", title2=\"FC Matrix 2\", cmap=\"turbo\", size=1):\n",
    "    \"\"\"\n",
    "    Plots two functional connectivity matrices side by side, each with its own colorbar.\n",
    "    \n",
    "    Parameters:\n",
    "        FC1 (ndarray): First N x N functional connectivity matrix.\n",
    "        FC2 (ndarray): Second N x N functional connectivity matrix.\n",
    "        title1 (str): Title for the first matrix.\n",
    "        title2 (str): Title for the second matrix.\n",
    "        cmap (str): Colormap for the heatmaps.\n",
    "        size (float): Scaling factor for figure size.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8 * size, 3.5 * size))\n",
    "\n",
    "    # Helper function to set ticks and labels for a given axis\n",
    "    def set_ticks_and_labels(ax, matrix):\n",
    "        n = matrix.shape[0]  # Number of regions\n",
    "        tick_positions = np.arange(n)  # Positions for ticks (0 to n-1)\n",
    "        # tick_labels = np.arange(1, n + 1)  # Labels starting from 1 to n\n",
    "        tick_labels = [str(tick_positions[0]+1)] + [''] * (len(tick_positions) - 2) + [str(tick_positions[-1]+1)]\n",
    "        \n",
    "        # Set ticks at the center of each square\n",
    "        ax.set_xticks(tick_positions)\n",
    "        ax.set_yticks(tick_positions)\n",
    "        \n",
    "        # Set tick labels\n",
    "        ax.set_xticklabels(tick_labels)\n",
    "        ax.set_yticklabels(tick_labels)\n",
    "\n",
    "    # Plot the first matrix\n",
    "    im1 = axes[0].imshow(FC1, cmap=cmap)  # , vmin=-1, vmax=1)\n",
    "    axes[0].set_title(title1)\n",
    "    axes[0].set_xlabel(\"Region Index\")\n",
    "    axes[0].set_ylabel(\"Region Index\")\n",
    "    cbar1 = fig.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "    cbar1.set_label(\"Connectivity Strength\")\n",
    "    set_ticks_and_labels(axes[0], FC1)\n",
    "\n",
    "    # Plot the second matrix\n",
    "    im2 = axes[1].imshow(FC2, cmap=cmap)  # , vmin=-1, vmax=1)\n",
    "    axes[1].set_title(title2)\n",
    "    axes[1].set_xlabel(\"Region Index\")\n",
    "    cbar2 = fig.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    cbar2.set_label(\"Connectivity Strength\")\n",
    "    set_ticks_and_labels(axes[1], FC2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def create_synthetic_C(N):\n",
    "    # Step 1: Create a symmetric random matrix\n",
    "    C = np.random.rand(N, N)  # Generate random values in [0, 1)\n",
    "    C = (C + C.T) / 2         # Ensure symmetry\n",
    "    np.fill_diagonal(C, np.random.rand(N))  # Randomize diagonal elements\n",
    "    \n",
    "    # Ensure all elements are positive (though they already should be)\n",
    "    C = np.abs(C)\n",
    "    \n",
    "    # Step 2: Modify elements within a 10% range\n",
    "    modified_C = C.copy()\n",
    "    for i in range(N):\n",
    "        for j in range(i, N):  # Only iterate over upper triangle (including diagonal)\n",
    "            delta = np.random.uniform(-0.1 * C[i, j], 0.1 * C[i, j])  # ±10% range\n",
    "            modified_C[i, j] += delta\n",
    "            # modified_C[j, i] = modified_C[i, j]  # Maintain symmetry\n",
    "            \n",
    "            # Ensure positivity\n",
    "            if modified_C[i, j] < 0:\n",
    "                modified_C[i, j] = 0\n",
    "                modified_C[j, i] = 0\n",
    "    \n",
    "    # Step 3: Normalize the matrix to sum to 0.2\n",
    "    total_sum = np.sum(modified_C)\n",
    "    normalized_C = modified_C / total_sum * 0.2\n",
    "    \n",
    "    return normalized_C\n",
    "\n",
    "def create_synthetic_C_diagonally_dominant(N):\n",
    "    # Step 1: Create a symmetric random matrix\n",
    "    A = np.random.rand(N, N)  # Generate random values in [0, 1)\n",
    "    A = (A + A.T) / 2         # Ensure symmetry\n",
    "    np.fill_diagonal(A, np.random.rand(N))  # Randomize diagonal elements\n",
    "    \n",
    "    # Ensure all elements are positive (though they already should be)\n",
    "    A = np.abs(A)\n",
    "    \n",
    "    # Step 2: Modify elements within a 10% range\n",
    "    modified_A = A.copy()\n",
    "    for i in range(N):\n",
    "        for j in range(i, N):  # Only iterate over upper triangle (including diagonal)\n",
    "            delta = np.random.uniform(-0.1 * A[i, j], 0.1 * A[i, j])  # ±10% range\n",
    "            modified_A[i, j] += delta\n",
    "            modified_A[j, i] = modified_A[i, j]  # Maintain symmetry\n",
    "            \n",
    "            # Ensure positivity\n",
    "            if modified_A[i, j] < 0:\n",
    "                modified_A[i, j] = 0\n",
    "                modified_A[j, i] = 0\n",
    "    \n",
    "    # Step 3: Ensure diagonal dominance\n",
    "    for i in range(N):\n",
    "        sum_off_diagonal = np.sum(modified_A[i, :]) - modified_A[i, i]  # Sum of non-diagonal elements\n",
    "        epsilon = 0.01  # Small positive value to ensure strict inequality\n",
    "        modified_A[i, i] = sum_off_diagonal + epsilon  # Adjust diagonal element\n",
    "    \n",
    "    # Step 4: Normalize the matrix to sum to 0.2\n",
    "    total_sum = np.sum(modified_A)\n",
    "    normalized_A = modified_A / total_sum * 0.2\n",
    "    \n",
    "    return normalized_A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64235cb5",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Create synthetic data to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5941761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate synthetic data\n",
    "NPARCELS = 20\n",
    "Tau = 1\n",
    "TR = 2\n",
    "\n",
    "gC = create_synthetic_C(NPARCELS)\n",
    "f_diff = np.random.uniform(low=0.035, high=0.065, size=NPARCELS)\n",
    "\n",
    "# Simulate an empirical FC and COVtau using some values for sigma:\n",
    "sigma_emp = np.random.uniform(low=0.01, high=0.25, size=NPARCELS)\n",
    "\n",
    "FCemp, COVemp, COVemptotal, A = [arr.copy() for arr in hopf_int(gC, f_diff, sigma_emp)]\n",
    "COVtauemp = (expm((Tau * TR) * A) @ COVemptotal)[:NPARCELS, :NPARCELS].copy()\n",
    "sigratiosim = np.zeros((NPARCELS, NPARCELS))\n",
    "for i in range(NPARCELS):\n",
    "    for j in range(NPARCELS):\n",
    "        sigratiosim[i, j] = 1 / np.sqrt(COVemp[i, i]) / np.sqrt(COVemp[j, j])\n",
    "COVtauemp *= sigratiosim\n",
    "\n",
    "# Extract the diagonal of the empirical covariance matrix\n",
    "COVemp_diag = np.diag(COVemp)\n",
    "\n",
    "### Plot synthetic data\n",
    "plt.imshow(gC)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(sigma_emp,'o-')\n",
    "plt.show()\n",
    "\n",
    "plot_FC_matrices(FCemp, COVtauemp, title1=\"FC Synthetic\", title2=\"COVtau Synthetic\", cmap=\"turbo\", size=1.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767a871e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f6777c",
   "metadata": {},
   "source": [
    "## Swarm-based optimization algorithm, such as Particle Swarm Optimization (PSO)\n",
    "Is a population-based method that can be used to solve optimization problems. PSO is particularly useful for non-convex or complex objective functions because it explores the search space using multiple \"particles\" that iteratively adjust their positions based on their own best-known position and the global best-known position.\n",
    "\n",
    "### Step 1: Define the Objective Function \n",
    "The goal is to minimize the difference between the calculated FC and the empirical FCemp. A common choice for such problems is the Frobenius norm of the difference between the two matrices: \n",
    "$$Error=∥FC−FCemp∥_F​ $$\n",
    "\n",
    "where $∥⋅∥_F$​ is the Frobenius norm. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5fa318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "\n",
    "### Objective: FC\n",
    "def objective_function_FC(sigma, gC, f_diff, FCemp, metric='fro'):\n",
    "    \"\"\"\n",
    "    Computes the Frobenius norm of the difference between the calculated FC and FCemp.\n",
    "    \n",
    "    Parameters:\n",
    "        sigma: Array of noise variances (3 components)\n",
    "        gC: Connectivity matrix\n",
    "        f_diff: Frequency differences\n",
    "        FCemp: Empirical functional connectivity matrix\n",
    "    \n",
    "    Returns:\n",
    "        Error: norm of the difference between FC and FCemp\n",
    "    \"\"\"\n",
    "    # Compute the calculated FC using the hopf_int function\n",
    "    FC, _, _, _ = hopf_int(gC, f_diff, sigma)\n",
    "        \n",
    "    ## Compute errors\n",
    "    ## Frobenius\n",
    "    if metric == 'fro':\n",
    "        error = np.linalg.norm(FC - FCemp, ord='fro')\n",
    "    ## MSE\n",
    "    if metric == 'mse':\n",
    "        error = np.mean((FC - FCemp) ** 2)  # Mean squared error for FC\n",
    "    ## MAE\n",
    "    if metric == 'mae':\n",
    "        error = np.mean(np.abs(FC - FCemp))  # Mean absolute error for FC\n",
    "\n",
    "    return error\n",
    "\n",
    "### Objective: FC and COVtau\n",
    "def objective_function_FC_COVtau(sigma, gC, f_diff, FCemp, COVtauemp, Tau, TR, metric='fro'):\n",
    "    \"\"\"\n",
    "    Computes the combined error for fitting both FC and COVtau.\n",
    "    \n",
    "    Parameters:\n",
    "        sigma: Array of noise variances (3 components)\n",
    "        gC: Connectivity matrix\n",
    "        f_diff: Frequency differences\n",
    "        FCemp: Empirical functional connectivity matrix\n",
    "        COVtauemp: Empirical covariance matrix at time lag Tau\n",
    "        Tau: Time lag for COVtau calculation\n",
    "        TR: Sampling time\n",
    "        metric: metrics to use\n",
    "            fro: Frobenius\n",
    "            mse: Mean Squared Error\n",
    "            mae: Mean Absolute Error\n",
    "    \n",
    "    Returns:\n",
    "        Total Error: Weighted sum of errors for FC and COVtau\n",
    "    \"\"\"\n",
    "\n",
    "    epsFC = 0.7\n",
    "    epsCOV = 0.3\n",
    "\n",
    "    ## Compute the simulated FC, COVsim, and COVtau\n",
    "    FCsim, COVsim, COVsimtotal, A = hopf_int(gC, f_diff, sigma)\n",
    "    N = gC.shape[0]\n",
    "    \n",
    "    ## Calculate COVtausim\n",
    "    COVtausim = (expm((Tau * TR) * A) @ COVsimtotal)[:N, :N].copy()\n",
    "    \n",
    "    ## Normalize COVtausim\n",
    "    sigratiosim = np.zeros((N, N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            sigratiosim[i, j] = 1 / np.sqrt(COVsim[i, i]) / np.sqrt(COVsim[j, j])\n",
    "    COVtausim *= sigratiosim\n",
    "    \n",
    "    ## Compute errors\n",
    "    ## Frobenius\n",
    "    if metric == 'fro':\n",
    "        fc_error = np.linalg.norm(FCsim - FCemp, ord='fro')  # Frobenius norm for FC\n",
    "        covtau_error = np.linalg.norm(COVtausim - COVtauemp, ord='fro')  # Frobenius norm for COVtau\n",
    "    ## MSE\n",
    "    if metric == 'mse':\n",
    "        fc_error = np.mean((FCsim - FCemp) ** 2)  # Mean squared error for FC\n",
    "        covtau_error = np.mean((COVtausim - COVtauemp) ** 2)  # Mean squared error for COVtau\n",
    "    ## MAE\n",
    "    if metric == 'mae':\n",
    "        fc_error = np.mean(np.abs(FCsim - FCemp))  # Mean absolute error for FC\n",
    "        covtau_error = np.mean(np.abs(COVtausim - COVtauemp))  # Mean absolute error for COVtau\n",
    "\n",
    "    ## Combine errors with weights\n",
    "    total_error = epsFC * fc_error + epsCOV * covtau_error\n",
    "\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6cb1a6",
   "metadata": {},
   "source": [
    "### Step 2: Implement the Particle Swarm Optimization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24874084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    def __init__(self, bounds, initial_position=None):\n",
    "        \"\"\"\n",
    "        Initialize a particle with either a specified initial position or a random position within bounds.\n",
    "        \n",
    "        Parameters:\n",
    "            bounds: List of tuples [(min, max), ...] specifying the bounds for each dimension.\n",
    "            initial_position: Optional array-like specifying the initial position of the particle.\n",
    "        \"\"\"\n",
    "        if initial_position is not None:\n",
    "            # Use the provided initial position (clipped to bounds)\n",
    "            self.position = np.array([\n",
    "                np.clip(initial_position[i], bounds[i][0], bounds[i][1]) for i in range(len(bounds))\n",
    "            ])\n",
    "        else:\n",
    "            # Generate a random position within bounds\n",
    "            self.position = np.array([\n",
    "                np.random.uniform(bounds[i][0], bounds[i][1]) for i in range(len(bounds))\n",
    "            ])\n",
    "        \n",
    "        # Small initial velocity\n",
    "        self.velocity = np.random.uniform(-0.1, 0.1, len(bounds))\n",
    "        \n",
    "        # Best position and value\n",
    "        self.best_position = self.position.copy()\n",
    "        self.best_value = float('inf')\n",
    "\n",
    "def pso(objective_function, bounds, num_particles=30, max_iter=100, w=0.5, c1=1.5, c2=1.5, args=(), initial_positions=None):\n",
    "    \"\"\"\n",
    "    Perform Particle Swarm Optimization to minimize the objective function.\n",
    "    \n",
    "    Parameters:\n",
    "        objective_function: The function to minimize.\n",
    "        bounds: List of tuples [(min, max), ...] specifying the bounds for each dimension.\n",
    "        num_particles: Number of particles in the swarm.\n",
    "        max_iter: Maximum number of iterations.\n",
    "        w: Inertia weight.\n",
    "        c1: Cognitive parameter (influence of particle's best position).\n",
    "        c2: Social parameter (influence of global best position).\n",
    "        args: Additional arguments to pass to the objective function.\n",
    "        initial_positions: Optional list of initial positions for the particles.\n",
    "                           If provided, overrides random initialization for those particles.\n",
    "    \n",
    "    Returns:\n",
    "        global_best_position: The best solution found.\n",
    "        global_best_value: The best objective value found.\n",
    "    \"\"\"\n",
    "    # Initialize the swarm\n",
    "    if initial_positions is not None:\n",
    "        # Use provided initial positions for the first few particles\n",
    "        swarm = [\n",
    "            Particle(bounds, initial_position=initial_positions[i]) if i < len(initial_positions) else Particle(bounds)\n",
    "            for i in range(num_particles)\n",
    "        ]\n",
    "    else:\n",
    "        # All particles are initialized randomly\n",
    "        swarm = [Particle(bounds) for _ in range(num_particles)]\n",
    "    \n",
    "    # Initialize global best\n",
    "    global_best_position = None\n",
    "    global_best_value = float('inf')\n",
    "    \n",
    "    # Iterate through generations\n",
    "    for iteration in range(max_iter):\n",
    "        for particle in swarm:\n",
    "            current_value = objective_function(particle.position, *args)\n",
    "            \n",
    "            if current_value < particle.best_value:\n",
    "                particle.best_value = current_value\n",
    "                particle.best_position = particle.position.copy()\n",
    "            \n",
    "            if current_value < global_best_value:\n",
    "                global_best_value = current_value\n",
    "                global_best_position = particle.position.copy()\n",
    "        \n",
    "        for particle in swarm:\n",
    "            r1, r2 = np.random.rand(len(bounds)), np.random.rand(len(bounds))\n",
    "            cognitive_component = c1 * r1 * (particle.best_position - particle.position)\n",
    "            social_component = c2 * r2 * (global_best_position - particle.position)\n",
    "            particle.velocity = w * particle.velocity + cognitive_component + social_component\n",
    "            \n",
    "            particle.velocity = np.clip(particle.velocity, -0.5, 0.5)\n",
    "            particle.position += particle.velocity\n",
    "            \n",
    "            # Ensure no element of position is zero\n",
    "            particle.position = np.array([\n",
    "                np.clip(particle.position[i], max(bounds[i][0], 1e-6), bounds[i][1]) for i in range(len(bounds))\n",
    "            ])\n",
    "        \n",
    "        print(f\"Iteration {iteration+1}/{max_iter}, Best Value: {global_best_value}\")\n",
    "    \n",
    "    return global_best_position, global_best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e96ac0",
   "metadata": {},
   "source": [
    "### Step 3: Run the PSO Algorithm\n",
    "#### Step 3.1: Set notmalization for the optimal_sigma\n",
    "\n",
    "One common approach is to use the empirical variance  of the signal (or its diagonal elements) as a reference for scaling. The diagonal elements of the covariance matrix (COVsim) correspond to the variances of the individual signals, and these are directly influenced by the scale of sigma. \n",
    "\n",
    "Let’s assume: \n",
    "\n",
    "    COVemp_diag = np.diag(COVemp) is the vector of empirical variances.\n",
    "    COVsim_diag = np.diag(COVsim) is the vector of simulated variances obtained from the model.\n",
    "     \n",
    "\n",
    "You can enforce a scaling constraint such that the sum of the simulated variances matches the sum of the empirical variances : \n",
    "$$ \\text{Scale Factor} = \\frac{\\sum \\ \\text{COVsim\\_diag}}{\\sum \\ \\text{COVemp\\_diag​}} $$\n",
    "\n",
    "Then\n",
    "$$ \\sigma_{\\text{norm}} = \\sigma_{\\text{optimal}} \\times \\sqrt{\\text{Scale Factor}} $$\n",
    "\n",
    "This ensures that the absolute scale of sigma is consistent with the empirical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff625982",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run PSO ######################################### \n",
    "# Initial guess for sigma\n",
    "sigma_init_guess = 0.05 * np.ones(NPARCELS)\n",
    "\n",
    "# Define bounds for sigma\n",
    "bound_min = 1e-4\n",
    "bound_max = 0.5\n",
    "bounds = [(bound_min, bound_max)] * NPARCELS\n",
    "\n",
    "# Initialization\n",
    "optimal_sigma = np.zeros(NPARCELS)\n",
    "# Generate initial positions with random perturbations within the specified percentage\n",
    "initial_positions = [sigma_init_guess for _ in range(NPARCELS)]\n",
    "optimal_value_max = 5e-3            # Maximum optimal value tolerance\n",
    "perturbation_percentage = 20        # Percentage in which sigma_init_guess is randomly modified in each Trial\n",
    "K_part = 10                         # Particles = K_part * NPARCELS\n",
    "K_w = 1                             # Inertia = K_w * w\n",
    "#\n",
    "metric = 'fro'                      # Select metric in objective function ('fro', 'mse', 'mae')\n",
    "#\n",
    "trial = 1\n",
    "while (np.any(optimal_sigma == 0) or optimal_value > optimal_value_max ):\n",
    "    print('--- Trial ', trial)\n",
    "    num_particles = K_part*NPARCELS\n",
    "    w = K_w*0.5\n",
    "    print('+ num_particles =', num_particles)\n",
    "    print('+ w =', w)\n",
    "    ## FC ##########################\n",
    "    optimal_sigma, optimal_value = pso(\n",
    "        objective_function=objective_function_FC,\n",
    "        bounds=bounds,\n",
    "        num_particles=num_particles,  # More particles --> better exploration\n",
    "        max_iter=50,                  # More iterations --> convergence\n",
    "        w=w,                          # Large inertia weight --> local exploration ; Small inertia weight --> global exploration\n",
    "        c1=1.5,\n",
    "        c2=1.5,\n",
    "        args=(gC, f_diff, FCemp, metric),\n",
    "        initial_positions=initial_positions\n",
    "    )\n",
    "    #################################\n",
    "    # ## FC + COVtau ##################\n",
    "    # optimal_sigma, optimal_value = pso(\n",
    "    #     objective_function=objective_function_FC_COVtau,\n",
    "    #     bounds=bounds,\n",
    "    #     num_particles=num_particles,  # More particles --> better exploration\n",
    "    #     max_iter=50,                  # More iterations --> convergence\n",
    "    #     w=w,                          # Large inertia weight --> local exploration ; Small inertia weight --> global exploration\n",
    "    #     c1=1.5,\n",
    "    #     c2=1.5,\n",
    "    #     args=(gC, f_diff, FCemp, COVtauemp, Tau, TR, metric),\n",
    "    #     initial_positions=initial_positions\n",
    "    # )\n",
    "    # #################################\n",
    "    # Set initial guess to last Trial\n",
    "    sigma_init_guess = optimal_sigma.copy()\n",
    "    # Generate initial positions with random perturbations within the specified percentage\n",
    "    # initial_positions = []\n",
    "    initial_positions = [\n",
    "        sigma_init_guess * (1 + np.random.uniform(-perturbation_percentage/100, perturbation_percentage/100, len(sigma_init_guess)))\n",
    "        for _ in range(K_part*NPARCELS)\n",
    "    ]\n",
    "    # Increase number of particles\n",
    "    if K_part < 40:\n",
    "        K_part += 5\n",
    "    else:\n",
    "        K_part += 2\n",
    "    # Increase global exploration (exit local minimum)\n",
    "    if w > 0.1:\n",
    "        K_w *= 0.9\n",
    "    trial += 1\n",
    "####################################################\n",
    "\n",
    "print('---')\n",
    "print(\"Optimized objective value:\", optimal_value)\n",
    "print(\"Optimized sigma:\", optimal_sigma)\n",
    "print('---')\n",
    "print(\"TEST FOR PROPORTIONALYTI:\")\n",
    "print(\"sigma_emp / optimal_sigma =\", sigma_emp / optimal_sigma)\n",
    "print(\"Multiplicative constant =\", np.mean(sigma_emp / optimal_sigma))\n",
    "\n",
    "# Compute the diagonal of the simulated covariance matrix using the optimized sigma\n",
    "_, COVsim, _, _ = hopf_int(gC, f_diff, optimal_sigma)\n",
    "COVsim_diag = np.diag(COVsim)\n",
    "# Normalize sigma to match the empirical variance\n",
    "sigma_sum_target = np.sum(COVemp_diag)  # Sum of empirical variances\n",
    "normalization_factor = sigma_sum_target / np.sum(COVsim_diag)  # Correct normalization factor\n",
    "sigma_normalized = optimal_sigma * np.sqrt(normalization_factor)\n",
    "print('---')\n",
    "print(\"Normalized sigma:\", sigma_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135e8320",
   "metadata": {},
   "source": [
    "### Step 4: Verify the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b158d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Comparison with TRUE sigma')\n",
    "plt.plot(range(1, NPARCELS+1), sigma_emp, 'o-', label='True sigma')\n",
    "plt.plot(range(1, NPARCELS+1), sigma_normalized, '.-', label='Optimal sigma normalized')\n",
    "plt.plot(range(1, NPARCELS+1), optimal_sigma, '.--', label='Optimal sigma')\n",
    "plt.xlabel('Parcels')\n",
    "ticks = np.arange(1, NPARCELS + 1)\n",
    "labels = [str(ticks[0])] + [''] * (len(ticks) - 2) + [str(ticks[-1])]\n",
    "plt.xticks(ticks,labels)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae11c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the final FC and COVtau using the sigma_normalized\n",
    "FC_final, COVsim, COVsimtotal, A = hopf_int(gC, f_diff, sigma_normalized)\n",
    "COVtausim = (expm((Tau * TR) * A) @ COVsimtotal)[:NPARCELS, :NPARCELS].copy()\n",
    "\n",
    "# Normalize COVtausim\n",
    "sigratiosim = np.zeros((NPARCELS, NPARCELS))\n",
    "for i in range(NPARCELS):\n",
    "    for j in range(NPARCELS):\n",
    "        sigratiosim[i, j] = 1 / np.sqrt(COVsim[i, i]) / np.sqrt(COVsim[j, j])\n",
    "COVtausim *= sigratiosim\n",
    "\n",
    "# Print results\n",
    "# print(\"Final FC:\")\n",
    "# print(FC_final)\n",
    "# print(\"Empirical FC:\")\n",
    "# print(FCemp)\n",
    "print(\"Difference (FC Frobenius norm):\", np.linalg.norm(FC_final - FCemp, ord='fro'))\n",
    "\n",
    "# print(\"\\nFinal COVtau:\")\n",
    "# print(COVtausim)\n",
    "# print(\"Empirical COVtau:\")\n",
    "# print(COVtauemp)\n",
    "print(\"Difference (COVtau Frobenius norm):\", np.linalg.norm(COVtausim - COVtauemp, ord='fro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c253711",
   "metadata": {},
   "outputs": [],
   "source": [
    "FCsim, COVsim, COVsimtotal, A = [arr.copy() for arr in hopf_int(gC, f_diff, sigma_normalized)]\n",
    "\n",
    "plot_FC_matrices(FCemp, FCsim, title1=\"FC Synthetic\", title2=\"FC Sim\", cmap=\"turbo\", size=1.2)\n",
    "plot_FC_matrices(COVtauemp, COVtausim, title1=\"COVtau Synthetic\", title2=\"COVtau Sim\", cmap=\"turbo\", size=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd66a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
